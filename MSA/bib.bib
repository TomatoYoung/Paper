@article{GANDHI2023424,
title = {Multimodal sentiment analysis: A systematic review of history, datasets, multimodal fusion methods, applications, challenges and future directions},
journal = {Information Fusion},
volume = {91},
pages = {424-444},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2022.09.025},
url = {https://www.sciencedirect.com/science/article/pii/S1566253522001634},
author = {Ankita Gandhi and Kinjal Adhvaryu and Soujanya Poria and Erik Cambria and Amir Hussain},
keywords = {Affective computing, Sentiment analysis, Multimodal fusion, Fusion techniques},
abstract = {Sentiment analysis (SA) has gained much traction In the field of artificial intelligence (AI) and natural language processing (NLP). There is growing demand to automate analysis of user sentiment towards products or services. Opinions are increasingly being shared online in the form of videos rather than text alone. This has led to SA using multiple modalities, termed Multimodal Sentiment Analysis (MSA), becoming an important research area. MSA utilises latest advancements in machine learning and deep learning at various stages including for multimodal feature extraction and fusion and sentiment polarity detection, with aims to minimize error rate and improve performance. This survey paper examines primary taxonomy and newly released multimodal fusion architectures. Recent developments in MSA architectures are divided into ten categories, namely early fusion, late fusion, hybrid fusion, model-level fusion, tensor fusion, hierarchical fusion, bi-modal fusion, attention-based fusion, quantum-based fusion and word-level fusion. A comparison of several architectural evolutions in terms of MSA fusion categories and their relative strengths and limitations are presented. Finally, a number of interdisciplinary applications and future research directions are proposed.}
}


@article{Mao_Zhang_Xu_Yuan_Liu_2024, 
title={Robust-MSA: Understanding the Impact of Modality Noise on Multimodal Sentiment Analysis}, 
volume={37}, url={https://ojs.aaai.org/index.php/AAAI/article/view/27078}, 
DOI={10.1609/aaai.v37i13.27078}, 
abstractNote={Improving model robustness against potential modality noise, as an essential step for adapting multimodal models to real-world applications, has received increasing attention among researchers. For Multimodal Sentiment Analysis (MSA), there is also a debate on whether multimodal models are more effective against noisy features than unimodal ones. Stressing on intuitive illustration and in-depth analysis of these concerns, we present Robust-MSA, an interactive platform that visualizes the impact of modality noise as well as simple defence methods to help researchers know better about how their models perform with imperfect real-world data.}, 
number={13}, 
journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
author={Mao, Huisheng and Zhang, Baozheng and Xu, Hua and Yuan, Ziqi and Liu, Yihe}, 
year={2024}, 
month={Jul.}, 
pages={16458-16460} 
}

@misc{zhang2023videollamainstructiontunedaudiovisuallanguage,
    title={Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding}, 
    author={Hang Zhang and Xin Li and Lidong Bing},
    year={2023},
    eprint={2306.02858},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2306.02858}, 
}

@misc{cheng2024videollama2advancingspatialtemporal,
    title={VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs}, 
    author={Zesen Cheng and Sicong Leng and Hang Zhang and Yifei Xin and Xin Li and Guanzheng Chen and Yongxin Zhu and Wenqi Zhang and Ziyang Luo and Deli Zhao and Lidong Bing},
    year={2024},
    eprint={2406.07476},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2406.07476}, 
}

@misc{wang2024qwen2vlenhancingvisionlanguagemodels,
    title={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution}, 
    author={Peng Wang and Shuai Bai and Sinan Tan and Shijie Wang and Zhihao Fan and Jinze Bai and Keqin Chen and Xuejing Liu and Jialin Wang and Wenbin Ge and Yang Fan and Kai Dang and Mengfei Du and Xuancheng Ren and Rui Men and Dayiheng Liu and Chang Zhou and Jingren Zhou and Junyang Lin},
    year={2024},
    eprint={2409.12191},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2409.12191}, 
}

@misc{goel2024omcatomnicontextaware,
    title={OMCAT: Omni Context Aware Transformer}, 
    author={Arushi Goel and Karan Sapra and Matthieu Le and Rafael Valle and Andrew Tao and Bryan Catanzaro},
    year={2024},
    eprint={2410.12109},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2410.12109}, 
}